<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Matt Bonakdarpour" />

<meta name="date" content="2016-01-21" />

<title>Simulating Discrete Markov Chains</title>

<script src="libs/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.1/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">

/* padding for bootstrap navbar */
body {
  padding-top: 50px;
  padding-bottom: 40px;
}


/* offset scroll position for anchor links (for fixed navbar)  */
.section h2 {
  padding-top: 55px;
  margin-top: -55px;
}
.section h3 {
  padding-top: 55px;
  margin-top: -55px;
}



/* don't use link color in navbar */
.dropdown-menu>li>a {
  color: black;
}

/* some padding for disqus */
#disqus_thread {
  margin-top: 45px;
}

</style>

<link rel="stylesheet" href="libs/font-awesome-4.1.0/css/font-awesome.min.css"/>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="libs/highlight/textmate.css"
      type="text/css" />
<script src="libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fiveMinuteStats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="license.html">License</a></li>
        <li><a href="https://github.com/stephens999/fiveMinuteStats">GitHub</a></li>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">
<h1 class="title">Simulating Discrete Markov Chains</h1>
<h4 class="author"><em>Matt Bonakdarpour</em></h4>
<h4 class="date"><em>2016-01-21</em></h4>
</div>

<div id="TOC">
<ul>
<li><a href="#pre-requisites">Pre-requisites</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#example-1">Example 1</a></li>
<li><a href="#example-2">Example 2</a><ul>
<li><a href="#a">2a</a></li>
<li><a href="#b">2b</a></li>
</ul></li>
</ul>
</div>

<p><strong>Last updated:</strong> 2016-01-23</p>
<p><strong>Code version:</strong> 1277966399366dd01b01a880d9352c360e10a3f5</p>
<div id="pre-requisites" class="section level1">
<h1>Pre-requisites</h1>
<p>This document assumes basic familiarity with Markov chains and linear algebra.</p>
</div>
<div id="overview" class="section level1">
<h1>Overview</h1>
<p>In this note, we will simulate a few Markov chains to gain some intuition about their behavior and how they evolve. Before moving on to concrete examples, we first settle on notation and implement an R function which will facilitates the simulations.</p>
<p>Let <span class="math">\(\pi^{(0)}\)</span> be our initial probability vector. For example, if we had a 3 state Markov chain with <span class="math">\(\pi^{(0)} = (0.5, 0.1, 0.4)\)</span>, this would tell us that our Markov chain has a 50% probability of starting in state 1, a 10% probability of starting in state 2, and a 40% probability of starting in state 3.</p>
<p>Let <span class="math">\(P\)</span> be our probability transition matrix. Recall from the note on discrete Markov chains that the probability vector after <span class="math">\(n\)</span> steps is equal to: <span class="math">\[\pi^{(n)} = \pi^{(0)}P^n\]</span> where <span class="math">\(P^n\)</span> is the matrix <span class="math">\(P\)</span> raised to the <span class="math">\(n\)</span>-th power.</p>
<p>With the facts above, we can simulate a Markov chain as follows:</p>
<ol style="list-style-type: decimal">
<li>Obtain probability transition matrix <span class="math">\(P\)</span><br /></li>
<li>Set <span class="math">\(P_n = P\)</span><br /></li>
<li>For t = 1…T:
<ol style="list-style-type: decimal">
<li>Set <span class="math">\(\pi^{(t)} = \pi^{(0)}P_n\)</span><br /></li>
<li>Let <span class="math">\(X_t\)</span> be a random draw from a Multinomial distribution with probability vector <span class="math">\(\pi^{(t)}\)</span>. This assigns a random state to <span class="math">\(X_t\)</span> according to our probability vector <span class="math">\(\pi^{(t)}\)</span>.<br /></li>
<li>Set <span class="math">\(P_n = P_nP\)</span>.</li>
</ol></li>
</ol>
<p>We implement this algorithm in the function below. Note that this function lets us run multiple Markov chains at once, and that we initialize <span class="math">\(\pi^{(0)}\)</span> so that the chain is forced to start in state 1 (i.e <span class="math">\(\pi^{(0)} = (1,0,0,\ldots,0)\)</span>).</p>
<pre class="r"><code># simulate a Markov chain
run.mc.sim &lt;- function( P,   # probability transition matrix
                        num.iters=50, 
                        num.chains=150 )
  {
  
  # number of possible states
  num.states &lt;- nrow(P)
  
  # states X_t for all chains
  states     &lt;- matrix(NA, ncol=num.chains, nrow=num.iters)
  
  # probability vectors pi^n through time
  all_probs  &lt;- matrix(NA, nrow=num.iters, ncol=num.states)
  
  # initial probability vector forces chain to start in state 1
  pi_0       &lt;- c(1, rep(0,num.states-1))

  # initialize variables for first state 
  P_n           &lt;- P
  all_probs[1,] &lt;- pi_0
  states[1,]    &lt;- 1
  
  for(t in 2:num.iters) {
    
    # probability vector for this iteration
    pi_n           &lt;- pi_0 %*% P_n
    all_probs[t,]  &lt;- pi_n
    
    # sample states for each chain
    for(p in seq_len(num.chains)) {
      states[t,p]   &lt;- which(rmultinom(1, 1, pi_n) == 1)
    }
    
    # update probability transition matrix
    P_n           &lt;- P_n %*% P
  }
  return(list(all.probs=all_probs, states=states))
}</code></pre>
</div>
<div id="example-1" class="section level1">
<h1>Example 1</h1>
<p>Assume our probability transition matrix is: <span class="math">\[P = \begin{bmatrix}
    0.7 &amp; 0.2 &amp; 0.1 \\
    0.4 &amp; 0.6 &amp; 0 \\
    0   &amp; 1   &amp; 0 
\end{bmatrix}\]</span></p>
<p>We initialize this matrix in R below:</p>
<pre class="r"><code># setup transition matrix 
P &lt;- t(matrix(c( 0.7, 0.2, 0.1, 
                0.4, 0.6,   0, 
                  0,   1,   0  ), nrow=3, ncol=3))</code></pre>
<p>Recall that the stationary distribution <span class="math">\(\pi\)</span> is the vector such that <span class="math">\[\pi = \pi P\]</span></p>
<p>We can find our stationary distribution by solving the following linear system: <span class="math">\[\begin{align*}
0.7\pi_1 + 0.4\pi_2  &amp;= \pi_1 \\
0.2\pi_1 + 0.6\pi_2 + \pi_3 &amp;= \pi_2 \\
0.1\pi_1 &amp;= \pi_3
\end{align*}\]</span> subject to <span class="math">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>. Putting these four equations together and moving all of the variables to the left hand side, we get the following linear system: <span class="math">\[\begin{align*}
-0.3\pi_1 + 0.4\pi_2  &amp;= 0 \\
0.2\pi_1 + -0.4\pi_2 + \pi_3 &amp;= 0 \\
0.1\pi_1 - \pi_3 &amp;= 0 \\
\pi_1 + \pi_2 + \pi_3 &amp;= 1
\end{align*}\]</span></p>
<p>We will define the linear system in matrix notation: <span class="math">\[\underbrace{\begin{bmatrix}
    -0.3 &amp;  0.4 &amp; 0  \\
     0.2 &amp; -0.4 &amp; 1  \\
     0.1 &amp;    0 &amp; 1  \\
     1   &amp;  1   &amp; 1 
\end{bmatrix}}_A \begin{bmatrix}
\pi_1 \\
\pi_2 \\
\pi_3 
\end{bmatrix} = \underbrace{\begin{bmatrix}
0 \\
0 \\ 
0 \\
1
\end{bmatrix}}_b \\
A\pi = b\]</span></p>
<p>Since this linear system has more equations than unknowns, it is called an <em>overdeterminted system</em>. Recall from linear algebra that an overdetermined system is consistent (i.e. we can solve for <span class="math">\(\pi\)</span> exactly) when <span class="math">\(b\)</span> is in the column space of <span class="math">\(A\)</span>. We can check this numerically by obtaining the rank of <span class="math">\(A\)</span>, then obtaining the rank of an augmented matrix with <span class="math">\(b\)</span> appended as a column of <span class="math">\(A\)</span>.</p>
<pre class="r"><code>library(Matrix)
A        &lt;- matrix(c(-0.3, 0.2, 0.1, 1, 0.4, -0.4, 0, 1, 0, 1, -1, 1 ), ncol=3,nrow=4)
b        &lt;- c(0,0,0, 1)
rank.A   &lt;- as.numeric(rankMatrix(A))
rank.Ab  &lt;- as.numeric(rankMatrix(cbind(A,b)))
print(paste(&quot;The rank of A =&quot;, rank.A, &quot;and the rank of the augmented matrix =&quot;, rank.Ab))</code></pre>
<pre><code>[1] &quot;The rank of A = 3 and the rank of the augmented matrix = 3&quot;</code></pre>
<p>We see that <span class="math">\(A\)</span> has full column rank, and that the rank is unchanged when we add <span class="math">\(b\)</span> as a column. Therefore, <span class="math">\(b\)</span> is in the column space of <span class="math">\(A\)</span>, and this system is consistent. We can find <span class="math">\(\pi\)</span> by solving the normal equations: <span class="math">\[A^TA\pi = A^Tb\]</span></p>
<p>We use the solve function in R to solve for the stationary distribution <span class="math">\(\pi\)</span>:</p>
<pre class="r"><code>pi        &lt;- drop(solve(t(A) %*% A, t(A) %*% b))
names(pi) &lt;- c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;)
pi </code></pre>
<pre><code>   state.1    state.2    state.3 
0.54054054 0.40540541 0.05405405 </code></pre>
<p>We find that: <span class="math">\[\begin{align*}
\pi_1 \approx 0.54, \pi_2 \approx 0.41, \pi_3 \approx 0.05
\end{align*}\]</span></p>
<p>Therefore, we expect the Markov chain to spend more time in states 1 and 2 as the chain evolves.</p>
<p>Now we will use the function we wrote in the previous section to check this result numerically.</p>
<pre class="r"><code>sim1 &lt;- run.mc.sim(P)</code></pre>
<p>Our function returns a list containing two matrices. The second matrix called “states”&quot; contains the states of each of our simulated chains through time. Recall that our state space is <span class="math">\(\{1,2,3\}\)</span>. Below, we first visualize how 5 of these chains evolve through time:</p>
<pre class="r"><code>states &lt;- sim1[[2]]
matplot(states[,1:5], type=&#39;l&#39;, lty=1, col=1:5, ylim=c(0,4), ylab=&#39;state&#39;, xlab=&#39;time&#39;)
abline(h=1, lty=3)
abline(h=3, lty=3)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-6-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>Using the notation from the first section, the first matrix we get from our function, “all_probs, contains <span class="math">\(\pi^{(n)}\)</span> through time. We can see how <span class="math">\(\pi^{(n)}\)</span> evolves as <span class="math">\(n\)</span> grows, and we can check if it converges to the stationary distribution we found above.</p>
<p>First we plot the time evolution of the probability of being in each state:</p>
<pre class="r"><code>all.probs &lt;- sim1[[1]]
matplot(all.probs, type=&#39;l&#39;, col=1:3, lty=1, ylab=&#39;probability&#39;, xlab=&#39;time&#39;)
legend(&#39;topright&#39;, c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;), lty=1, col=1:3)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-7-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>Indeed, we see that these probabilities quickly converge. Just by eye-balling the plot, we can see that the final probabilities are about equal to the vector <span class="math">\(\pi\)</span> we found above.</p>
<p>By inspecting the actual values, we can confirm that the values of <span class="math">\(\pi^{(n)}\)</span> converge to the vector <span class="math">\(\pi\)</span> exactly. The first row in the matrix below is from the simulation, and the second row is the quantity we obtained by solving the normal equations:</p>
<pre class="r"><code>results1           &lt;- t(data.frame(pi_n = all.probs[50,], pi = pi))
colnames(results1) &lt;- c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;)
results1</code></pre>
<pre><code>       state.1   state.2    state.3
pi_n 0.5405405 0.4054054 0.05405405
pi   0.5405405 0.4054054 0.05405405</code></pre>
<p>Finally, we can also plot the proportion of chains that are in each state through time. These should roughly equal the probability vectors above, with some noise due to random chance:</p>
<pre class="r"><code>state.probs &lt;- t(apply(apply(sim1[[2]], 1, function(x) table(factor(x, levels=1:3))), 2, function(x) x/sum(x)))
matplot(state.probs[1:50,], col=1:3, lty=1, type=&#39;l&#39;, ylab=&#39;empirical probability&#39;, xlab=&#39;time&#39;)
legend(&#39;topright&#39;, c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;), lty=1, col=1:3)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-9-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2" class="section level1">
<h1>Example 2</h1>
<div id="a" class="section level2">
<h2>2a</h2>
<p>Next we will quickly do two similar, but larger, experiments with the size of our state space equal to 8. Assume our probability transition matrix is: <span class="math">\[P = \begin{bmatrix}
    0.33 &amp; 0.66 &amp; 0     &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0.33 &amp; 0.33 &amp; 0.33  &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0.33 &amp; 0.33 &amp; 0.33 &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0.33 &amp; 0.33 &amp; 0.33 &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0    &amp; 0.33 &amp; 0.33 &amp; 0.33  &amp; 0    &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.33 &amp; 0.33  &amp; 0.33 &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.33  &amp; 0.33 &amp; 0.33 \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0     &amp; 0.66 &amp; 0.33 \\
\end{bmatrix}\]</span></p>
<p>We next initialize our transition matrix in R:</p>
<pre class="r"><code>P &lt;- t(matrix(c( 1/3, 2/3,   0,   0,  0,   0,   0,   0,
                 1/3, 1/3, 1/3,   0,  0,   0,   0,   0,
                   0, 1/3, 1/3, 1/3,  0,   0,   0,   0,
                   0,   0, 1/3, 1/3, 1/3,  0,   0,   0,
                   0,   0,   0, 1/3, 1/3, 1/3,  0,   0,
                   0,   0,   0,   0, 1/3, 1/3, 1/3,  0,
                   0,   0,   0,   0,   0, 1/3, 1/3, 1/3,
                   0,   0,   0,   0,   0,   0, 2/3, 1/3), nrow=8, ncol=8))</code></pre>
<p>After briefly studying this matrix, we can see that for states 2 through 7, this transition matrix forces the chain to either stay in the current state or move one state up or down, all with equal probability. For the edge cases, states 1 and 8, the chain can either stay or reflect towards the middle states. Since it’s “easier” to get to one of the middle states (either from above or below), we should see that the probabilities for these states converge to a higher number than the states on the boundaries.</p>
<p>Now we run our simulations with the transition matrix above:</p>
<pre class="r"><code>sim2a &lt;- run.mc.sim(P)</code></pre>
<p>First we plot 5 of the chains through time below:</p>
<pre class="r"><code>states &lt;- sim2a[[2]]
matplot(states[,1:5], type=&#39;l&#39;, lty=1, col=1:5, ylim=c(0,9), ylab=&#39;state&#39;, xlab=&#39;time&#39;)
abline(h=1, lty=3)
abline(h=8, lty=3)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-12-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>Next we inpsect <span class="math">\(\pi^{(n)}\)</span> through time:</p>
<pre class="r"><code>all.probs &lt;- sim2a[[1]]
matplot(all.probs, type=&#39;l&#39;, col=1:8, lty=1, ylab=&#39;probability&#39;,
        xlab=&#39;time&#39;, ylim=c(0, 0.5))
legend(&#39;topright&#39;, paste(&#39;state.&#39;, 1:8, sep=&#39;&#39;), lty=1, col=1:8)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-13-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /> These results match our intuition above. The probability of being in states 1 and 8 converge to smaller values than the others.</p>
</div>
<div id="b" class="section level2">
<h2>2b</h2>
<p>Now we alter the transition matrix above to encourage the chain to stay in states 4 and 5: <span class="math">\[P = \begin{bmatrix}
    0.33 &amp; 0.66 &amp; 0     &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0.33 &amp; 0.33 &amp; 0.33  &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0.08 &amp; 0.08 &amp; 0.84 &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0.08 &amp; 0.84 &amp; 0.08 &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0    &amp; 0.08 &amp; 0.84 &amp; 0.08  &amp; 0    &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.84 &amp; 0.08  &amp; 0.08 &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.33  &amp; 0.33 &amp; 0.33 \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0     &amp; 0.66 &amp; 0.33 \\
\end{bmatrix}\]</span></p>
<p>and initialize the transition matrix in R:</p>
<pre class="r"><code>P &lt;- t(matrix(c( 1/3,   2/3,    0,    0,    0,    0,    0,   0,
                 1/3,   1/3,  1/3,    0,    0,    0,    0,   0,
                   0,  .5/6, .5/6,  5/6,    0,    0,    0,   0,
                   0,     0, .5/6,  5/6, .5/6,    0,    0,   0,
                   0,     0,    0, .5/6,  5/6, .5/6,    0,   0,
                   0,     0,    0,    0,  5/6, .5/6, .5/6,   0,
                   0,     0,    0,    0,    0,  1/3,  1/3, 1/3,
                   0,     0,    0,    0,    0,    0,  2/3, 1/3 ), nrow=8, ncol=8))</code></pre>
<pre class="r"><code>sim2b &lt;- run.mc.sim(P)</code></pre>
<p>Below we inspect <span class="math">\(\pi^{(n)}\)</span> through time and see that the probability vector converges to a vector placing most of the probability mass on states 4 and 5.</p>
<pre class="r"><code>all.probs &lt;- sim2b[[1]]
matplot(all.probs, type=&#39;l&#39;, col=1:8, lty=1, ylab=&#39;probability&#39;,
        xlab=&#39;time&#39;, ylim=c(0,1.2))
legend(&#39;topright&#39;, paste(&#39;state.&#39;, 1:8, sep=&#39;&#39;), lty=1, col=1:8)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-16-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>Finally we confirm that our empirical probability also exhibit similar behavior:</p>
<pre class="r"><code>state.probs &lt;- t(apply(apply(sim2b[[2]], 1, function(x) table(factor(x, levels=1:8))), 2, function(x) x/sum(x)))
matplot(state.probs[1:50,], col=1:8, lty=1, type=&#39;l&#39;, ylab=&#39;empirical probability&#39;, xlab=&#39;time&#39;, ylim=c(0,1.2))
legend(&#39;topright&#39;, paste(&#39;state.&#39;, 1:8, sep=&#39;&#39;), lty=1, col=1:8)</code></pre>
<p><img src="figure/simulate_discrete_chains.Rmd/unnamed-chunk-17-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /> # Session information</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.2.2 (2015-08-14)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.5 (Yosemite)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] Matrix_1.2-3 knitr_1.11  

loaded via a namespace (and not attached):
 [1] magrittr_1.5    formatR_1.2.1   tools_3.2.2     htmltools_0.3  
 [5] yaml_2.1.13     stringi_1.0-1   rmarkdown_0.9.2 grid_3.2.2     
 [9] stringr_1.0.0   digest_0.6.8    lattice_0.20-33 evaluate_0.8   </code></pre>
</div>
</div>


<!-- some extra javascript for older browsers -->
<script type="text/javascript" src="libs/polyfill.js"></script>

<script>

// manage active state of menu based on current page
$(document).ready(function () {

    // active menu
    href = window.location.pathname
    href = href.substr(href.lastIndexOf('/') + 1)
    $('a[href="' + href + '"]').parent().addClass('active');

    // manage active menu header
    if (href.startsWith('authoring_'))
      $('a[href="' + 'authoring' + '"]').parent().addClass('active');
    else if (href.endsWith('_format.html'))
      $('a[href="' + 'formats' + '"]').parent().addClass('active');
    else if (href.startsWith('developer_'))
      $('a[href="' + 'developer' + '"]').parent().addClass('active');

});

</script>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
