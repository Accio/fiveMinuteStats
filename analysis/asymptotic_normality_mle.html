<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Matt Bonakdarpour" />

<meta name="date" content="2016-01-14" />

<title>Asymptotic Normality of MLE</title>

<script src="libs/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.1/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">

/* padding for bootstrap navbar */
body {
  padding-top: 50px;
  padding-bottom: 40px;
}


/* offset scroll position for anchor links (for fixed navbar)  */
.section h2 {
  padding-top: 55px;
  margin-top: -55px;
}
.section h3 {
  padding-top: 55px;
  margin-top: -55px;
}



/* don't use link color in navbar */
.dropdown-menu>li>a {
  color: black;
}

/* some padding for disqus */
#disqus_thread {
  margin-top: 45px;
}

</style>

<link rel="stylesheet" href="libs/font-awesome-4.1.0/css/font-awesome.min.css"/>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="libs/highlight/textmate.css"
      type="text/css" />
<script src="libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fiveMinuteStats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="license.html">License</a></li>
        <li><a href="https://github.com/stephens999/fiveMinuteStats">GitHub</a></li>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">
<h1 class="title">Asymptotic Normality of MLE</h1>
<h4 class="author"><em>Matt Bonakdarpour</em></h4>
<h4 class="date"><em>2016-01-14</em></h4>
</div>

<div id="TOC">
<ul>
<li><a href="#main-result">Main Result</a><ul>
<li><a href="#fisher-information">Fisher Information</a></li>
</ul></li>
<li><a href="#example-1-bernoulli-proportion">Example 1: Bernoulli Proportion</a></li>
<li><a href="#example-2-poisson-mean">Example 2: Poisson Mean</a></li>
</ul>
</div>

<p><strong>Last updated:</strong> 2016-01-20</p>
<p><strong>Code version:</strong> b20215f3e4166633354253da20cfc2359d4b325c</p>
<div id="main-result" class="section level2">
<h2>Main Result</h2>
<p>Maximum likelihood is a popular method for estimating parameters in a statistical model. Assume we observe i.i.d. samples <span class="math">\(X_1,\ldots,X_n\)</span> with probability distribution governed by the parameter <span class="math">\(\theta\)</span>. Let <span class="math">\(\theta_0\)</span> be the true value of <span class="math">\(\theta\)</span>, and <span class="math">\(\hat{\theta}\)</span> be the maximum likelihood estimate (MLE). Under regularity conditions, the MLE for <span class="math">\(\theta\)</span> is asympototically normal with mean <span class="math">\(\theta_0\)</span> and variance <span class="math">\(I^{-1}(\theta_0)\)</span>. <span class="math">\(I(\theta_0)\)</span> is called the <strong>Fisher information</strong> – we will describe it below. Precisely, this result states that: <span class="math">\[\sqrt{n}(\hat{\theta} - \theta_0) \rightarrow N(0,I^{-1}(\theta_0))\]</span></p>
<p>If <span class="math">\(\hat{\theta}\)</span> is the MLE, then this says that <span class="math">\((\hat{\theta} - \theta_0)/I^{-1}(\theta_0)\)</span> is nearly <span class="math">\(N(0,1)\)</span> when the sample size <span class="math">\(n\)</span> is large. This allows us to construct approximate confidence intervals for <span class="math">\(\theta\)</span> and perform hypothesis tests.</p>
<div id="fisher-information" class="section level3">
<h3>Fisher Information</h3>
<p>First, some notation:<br />* <strong>Likelihood</strong>: <span class="math">\(L(\theta) = p(X_1,\ldots,X_n;\theta)\)</span><br />* <strong>Log-likelihood</strong>: <span class="math">\(\ell(\theta) = \log{L(\theta)}\)</span><br />* <strong>Score function</strong>: <span class="math">\(s(\theta) = \frac{d}{d\theta}\ell(\theta)\)</span></p>
<p>The MLE <span class="math">\(\hat{\theta}\)</span> maximizes both <span class="math">\(L(\theta)\)</span> and <span class="math">\(\ell(\theta)\)</span>. We typically find <span class="math">\(\hat{\theta}\)</span> by differentiation, solving the following equation for <span class="math">\(\theta\)</span>:<br /><span class="math">\[s(\theta) = 0\]</span></p>
<p>Under regularity conditions, the Fisher information, <span class="math">\(I(\theta)\)</span>, is : <span class="math">\[I(\theta) = E_{\theta}\left[-\frac{d^2}{d\theta^2}\ell(\theta)\right]\]</span></p>
<p>Intuitively, this quantity tells us, on average, how peaked the likelihood function is. The more peaked the likelihood function, the “better” we know the true parameter. In this way, this quantity provides <em>information</em> about the true parameter.</p>
</div>
</div>
<div id="example-1-bernoulli-proportion" class="section level2">
<h2>Example 1: Bernoulli Proportion</h2>
<p>Assume we observe i.i.d. samples <span class="math">\(X_1,\ldots,X_n\)</span> drawn from a Bernoulli distribution with true parameter <span class="math">\(p_0\)</span>. Given, these observations, the log-likelihood is: <span class="math">\[\ell(p) = \sum X_i\log{p} + (1-X_i)\log(1-p)\]</span> Setting the derivative equal to zero, we obtain:<br /><span class="math">\[\frac{d}{dp}\ell(p) = \sum \frac{X_i}{p} - \frac{(1-X_i)}{1-p} = 0\]</span><br />Solving for <span class="math">\(p\)</span>, we get that the MLE is the sample mean: <span class="math">\(\hat{p} = \bar{X}\)</span>.</p>
<p>The second derivative with respect to p is:<br /><span class="math">\[\frac{d^2}{dp^2} \ell(p) = \sum -\frac{X_i}{p^2} - \frac{(1-X_i)}{(1-p)^2}\]</span></p>
<p>The Fisher information is therefore: <span class="math">\[I(p) = E\left[-\frac{d^2}{dp^2}\ell(p)\right] = -\frac{E[X_i]}{p^2} - \frac{(1-E[X_i])}{(1-p)^2} = \frac{1}{p(1-p)}\]</span></p>
<p>From the result at the top of the page, we have that (for large n), <span class="math">\(\hat{p}\)</span> is approximately <span class="math">\(N\left(p,\frac{p(1-p)}{n}\right)\)</span>. We illustrate this approximation in the simulation below.</p>
<p>The simulation creates 7000 different sets of samples. In each sample, we have 100 draws from a Bernoulli distribution with true parameter equal to 0.4. We compute the MLE separately for each sample and plot a histogram of these 7000 MLEs. On top of this histogram, we plot the density of the asymptotic distribution as a solid line.</p>
<pre class="r"><code>num.iterations         &lt;- 7000
p.truth                &lt;- 0.4
num.samples.per.iter   &lt;- 100
samples                &lt;- numeric(num.iterations)
for(iter in seq_len(num.iterations)) {
  samples[iter] &lt;- mean(rbinom(num.samples.per.iter, 1, p.truth))
}
hist(samples, freq=F)
curve(dnorm(x, mean=p.truth,sd=sqrt((p.truth*(1-p.truth)/num.samples.per.iter) )), .25, .55, lwd=2, xlab = &quot;&quot;, ylab = &quot;&quot;, add = T)</code></pre>
<p><img src="figure/asymptotic_normality_mle.Rmd/unnamed-chunk-1-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2-poisson-mean" class="section level2">
<h2>Example 2: Poisson Mean</h2>
<p>Assume we observe i.i.d. samples <span class="math">\(X_1,\ldots,X_n\)</span> drawn from a Poisson distribution with true parameter <span class="math">\(\lambda\)</span>. The log-likelihood is:</p>
<p><span class="math">\[ \ell(\lambda; X_1,\ldots,X_n) = \sum_{i=1}^n -\lambda + X_i\log(\lambda) + \log(X_i!)\]</span></p>
<p>Taking the derivative with respect to <span class="math">\(\lambda\)</span>, setting it equal to zero, and solving for <span class="math">\(\lambda\)</span> gives us the estimate <span class="math">\(\hat{lambda} = \frac{1}{n}\sum_{i=1}^{n}X_i = \bar{X}\)</span>. The Fisher information is:</p>
<p><span class="math">\[ E_{\lambda}\left[-\frac{d^2}{d\lambda^2}\ell(\lambda)\right] = E[\frac{X}{\lambda^2}] = \frac{1}{\lambda}\]</span></p>
<p>So we have that, we have that (for large n), <span class="math">\(\hat{\lambda}\)</span> is approximately <span class="math">\(N\left(\lambda,\frac{1}{n\lambda}\right)\)</span>. We illustrate this in the same was as above:</p>
<pre class="r"><code>num.iterations         &lt;- 7000
lambda.truth           &lt;- 0.8
num.samples.per.iter   &lt;- 100
samples                &lt;- numeric(num.iterations)
for(iter in seq_len(num.iterations)) {
  samples[iter] &lt;- mean(rpois(num.samples.per.iter, lambda.truth))
}
hist(samples, freq=F)
curve(dnorm(x, mean=lambda.truth,sd=sqrt(lambda.truth/num.samples.per.iter) ), 0.4, 1.2, lwd=2, xlab = &quot;&quot;, ylab = &quot;&quot;, add = T)</code></pre>
<p><img src="figure/asymptotic_normality_mle.Rmd/unnamed-chunk-2-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.2.2 (2015-08-14)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.5 (Yosemite)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] knitr_1.11

loaded via a namespace (and not attached):
 [1] magrittr_1.5    formatR_1.2.1   tools_3.2.2     htmltools_0.3  
 [5] yaml_2.1.13     stringi_1.0-1   rmarkdown_0.9.2 stringr_1.0.0  
 [9] digest_0.6.8    evaluate_0.8   </code></pre>
</div>


<!-- some extra javascript for older browsers -->
<script type="text/javascript" src="libs/polyfill.js"></script>

<script>

// manage active state of menu based on current page
$(document).ready(function () {

    // active menu
    href = window.location.pathname
    href = href.substr(href.lastIndexOf('/') + 1)
    $('a[href="' + href + '"]').parent().addClass('active');

    // manage active menu header
    if (href.startsWith('authoring_'))
      $('a[href="' + 'authoring' + '"]').parent().addClass('active');
    else if (href.endsWith('_format.html'))
      $('a[href="' + 'formats' + '"]').parent().addClass('active');
    else if (href.startsWith('developer_'))
      $('a[href="' + 'developer' + '"]').parent().addClass('active');

});

</script>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
