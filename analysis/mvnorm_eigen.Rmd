---
title: "Interpreting multivariate normal via eigen-decomposition of covariance matrix"
author: "Matthew Stephens"
date: "2021-02-28"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r}
library(mvtnorm)
```

## Pre-requisites

You should be familiar with the 
[Multivariate normal distribution](mvnorm.html), and with the eigen-decomposition for symmetric positive semi-definite (PSD) matrices.

## Introduction

Getting an intuition for what the $p$-dimensional multivariate normal distribution, $N_p(\mu,\Sigma)$,  "looks like" can be difficult. For $p=1,2$ things are not too bad: we can directly visualize a univariate normal distribution by plotting its density, and visualize a bivariate normal distribution by plotting a contour plot of the density, or by simulating samples from the distribution and visualizing them using a 2d scatterplot. For example, the following code does this for
$N(0,\Sigma)$ where 
$$\Sigma = \begin{pmatrix}
1.0 & 0.7 \\ 0.7 & 1.0 \end{pmatrix}$$:
```{r}
Sigma= cbind(c(1,0.7),c(0.7,1))
X = rmvnorm(1000,c(0,0),Sigma)
plot(X[,1],X[,2],main="Samples from bivariate normal with variance Sigma",asp=1)
```
But in $p=100$ dimensions, or even just $p=4$ dimensions, things become much harder because direct visualization is impractical.

So how can we get intuition about the 
multivariate normal distribution, $N_p(\mu,\Sigma)$
when $p$ is large?

Note first that the mean $\mu$ is just a vector of $p$ numbers, and generally
causes few problem in interpretation: you can just think of each number as specifying the mean in each of the $p$ coordinates one at a time.

 In contrast, the covariance matrix $\Sigma$ is a $p \times p$ matrix that captures potentially more complex patterns, and creates more 
 challenges for intuition. One possible approach is to plot a heatmap of this matrix, and this can certainly be helpful in certain situations. However, this vignette describes a more algebraic approach, based on the eigen-decomposition of $\Sigma$. 

## Some linear algebra

Recall that any valid $p \times p$ covariance matrix $\Sigma$ must be
symmetric and positive semi-definite (PSD). Furthermore, recall that
any such PSD matrix must have eigen-decomposition:
$$\Sigma = V \Lambda V'$$ 
where:

  - $\Lambda$ is a $K \times K$ diagonal matrix with the non-zero eigenvalues of $\Sigma$, $\lambda_1,\dots,\lambda_K$ say, on the diagonal ($K \leq p$ is the rank of $\Sigma$).

  - $V$ is a $p \times K$ orthonormal matrix ($V'V=I_K$), whose columns $v_1,\dots,v_K$ are the normalized eigenvectors of $\Sigma$ corresponding to the non-zero eigenvalues.
  
Recall also that if $Z \sim N_p(0, I_p)$ and $A$ is any $n \times p$ matrix then $\mu + AZ \sim N(\mu, AA')$.

Now apply this last result with $A= V \Lambda^{0.5}$ where $\Lambda^{0.5}$ is the diagonal matrix with $\lambda_1^{0.5},\dots,\lambda_K^{0.5}$ on the diagonal. We get
$$ \mu + V \Lambda^{0.5} Z \sim N_p(\mu, V \Lambda^{0.5} \Lambda^{0.5} V').$$
That is,
$$\mu + V \Lambda^{0.5} Z \sim N_p(\mu, \Sigma).$$
We can write the matrix multiple $V\Lambda^{0.5} Z$ as a sum to make the structure more obvious:
$$\mu + \sum_{k=1}^K \lambda_k^{0.5} z_k v_k  \sim N_p(\mu, \Sigma).$$
Here $\mu$ and $v_1,\dots,v_K$ are all column vector of length $p$,
whereas the $\lambda_k$ and $z_k$ are all scalars.


### Interpration as a random linear combination of eigenvectors

From this algebra, if $X \sim N_p(\mu,\Sigma)$, then we can think of $X$
as being generated by taking the mean $\mu$, and adding a *random linear combination* of the eigenvectors
of $\Sigma$. Specifically 
$$X = \mu + \sum_{k=1}^K b_k v_k,$$ 
where the weights 
$$b_k=\lambda_k^{0.5} z_k \sim N(0,\lambda_k).$$
are independent of one another. 

Note that if $\lambda_k$ is small then $b_k \approx 0$, 
so the eigenvectors with small eigenvalues contribute little to $X$, 
and we can focus on the eigenvectors with large eigenvalues.
Indeed, this approach provides the simplest insights when most of the $\lambda_k$ are negligible, and only one or two eigenvectors
contribute meaningfully to the sum.

## Example: rank 1 covariance

To make a simple example, set $\mu=0$ and assume
 $\Sigma$ is a rank 1 matrix. That is, $\Sigma$ 
 has only one eigenvector:
$$\Sigma = \lambda vv'$$
for some $p$-vector $v$.

In this case the algebra above gives the representation
$X= b v$ where $b \sim N(0,\lambda)$. That is $X$ is simply a multiple of $v$, where the multiplier is randomly distributed from a univariate normal. Thus in this case the randomness in $X$ boils down to the randomness in a single random univarate normal, which is easy  to visualize. 

To give a specific example, suppose that $v$ is the vector of all 1s $v=(1,\dots,1)$ and $\lambda=1$. That is $\Sigma$ is a matrix of all 1s.
Then $X= (b,b,b,\dots,b)$ where $b \sim N(0,1)$.

To give another specific example, if $v=(-1,-1,-1,1,1)$ and $\lambda=2$ then $X= (-b,-b,-b,b,b)$ where $b \sim N(0,2)$.



