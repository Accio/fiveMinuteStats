<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Matt Bonakdarpour [cre], Joshua Bon [ctb], Matthew Stephens [ctb]" />

<meta name="date" content="2019-03-30" />

<title>Asymptotic Normality of MLE</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fiveMinuteStats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/stephens999/fiveMinuteStats">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Asymptotic Normality of MLE</h1>
<h4 class="author"><em>Matt Bonakdarpour [cre], Joshua Bon [ctb], Matthew Stephens [ctb]</em></h4>
<h4 class="date"><em>2019-03-30</em></h4>

</div>


<div id="prerequisites" class="section level2">
<h2>Prerequisites</h2>
<p>You should be familiar with the concept of <a href="likelihood_function.html">Likelihood function</a>.</p>
</div>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>Maximum likelihood estimation is a popular method for estimating parameters in a statistical model. As its name suggests, maximum likelihood estimation involves finding the value of the parameter that maximizes the likelihood function (or, equivalently, maximizes the log-likelihood function). This value is called the maximum likelihood estimate, or MLE.</p>
<p>It seems natural to ask about the accuracy of an MLE: how far from the “true” value of the parameter can we expect the MLE to be? This vignette answers this question in a simple but important case: maximum likelihood estimation based on independent and identically distributed (i.i.d.) data from a model.</p>
</div>
<div id="asymptotic-distribution-of-mle-for-i.i.d.-data" class="section level2">
<h2>Asymptotic distribution of MLE for i.i.d. data</h2>
<div id="set-up" class="section level3">
<h3>Set-up</h3>
<p>Suppose we observe <em>independent and identically distributed</em> (i.i.d.) samples <span class="math inline">\(X_1,\ldots,X_n\)</span> from a probability distribution <span class="math inline">\(p(\cdot; \theta)\)</span> governed by a parameter <span class="math inline">\(\theta\)</span> that is to be estimated. Then the likelihood for <span class="math inline">\(\theta\)</span> is: <span class="math display">\[L(\theta; X_1,\dots,X_n) := p(X_1,\ldots,X_n;\theta) = \prod_{i=1}^n p(X_i ; \theta).\]</span> And the log-likelihood is: <span class="math display">\[\ell(\theta; X_1,\dots,X_n):= \log L(\theta;X_1,\dots,X_n) = \sum_{i=1}^n \log p(X_i; \theta).\]</span> Let <span class="math inline">\(\theta_0\)</span> denote the true value of <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(\hat{\theta}\)</span> denote the maximum likelihood estimate (MLE). Because <span class="math inline">\(\ell\)</span> is a monotonic function of <span class="math inline">\(L\)</span> the MLE <span class="math inline">\(\hat{\theta}\)</span> maximizes both <span class="math inline">\(L\)</span> and <span class="math inline">\(\ell\)</span>. (In simple cases we typically find <span class="math inline">\(\hat{\theta}\)</span> by differentiating the log-likelihood and solving <span class="math inline">\(\ell&#39;(\theta;X_1,\dots,X_n)=0\)</span>.)</p>
</div>
<div id="result" class="section level3">
<h3>Result</h3>
<p>Under some technical conditions that often hold in practice (often referred to as “regularity conditions”), and for <span class="math inline">\(n\)</span> sufficiently large, we have the following approximate result: <span class="math display">\[{\hat{\theta}} {\dot\sim} N(\theta_0,I_{n}(\theta_0)^{-1})\]</span> where the precision (inverse variance), <span class="math inline">\(I_n(\theta_0)\)</span>, is a quantity known as the <strong>Fisher information</strong>, which is defined as <span class="math display">\[I_{n}(\theta) := E_{\theta}\left[-\frac{d^2}{d\theta^2}\ell(\theta; X_1,\dots,X_n)\right].\]</span> Notes:</p>
<ul>
<li>the notation <span class="math inline">\(\dot\sim\)</span> means “approximately distributed as”.</li>
<li>the expectation in the definition of <span class="math inline">\(I_n\)</span> is with respect to the distribution of <span class="math inline">\(X_1,\dots,X_n\)</span>, <span class="math inline">\(p(X_1,\dots,X_n | \theta)\)</span>.</li>
<li>The (negative) second derivative measures the curvature of a function, and so one can think of <span class="math inline">\(I_{n}(\theta)\)</span> as measuring, on average, how curved the log-likelihood function is – or, in other words, how peaked the likelihood function is. The more peaked the likelihood function, the more <em>information</em> it contains, and the more precise the MLE will be.</li>
</ul>
<p>With i.i.d. data the Fisher information can be easily shown to have the form <span class="math display">\[I_n(\theta) = n I(\theta)\]</span> where <span class="math inline">\(I(\theta)\)</span> is the Fisher information for a {} observation - that is, <span class="math inline">\(I(\theta) = I_1(\theta)\)</span>. [This follows directly from the fact that <span class="math inline">\(\ell\)</span> is the sum of <span class="math inline">\(n\)</span> terms, and then applying linearity of expectation: Exercise!]</p>
</div>
</div>
<div id="interpretation" class="section level2">
<h2>Interpretation</h2>
<div id="sampling-distribution" class="section level3">
<h3>Sampling distribution</h3>
<p>The interpretation of this result needs a little care. In particular, it is important to understand what it means to say that the MLE has a “distribution”, since for any given dataset <span class="math inline">\(X_1,\dots,X_n\)</span> the MLE <span class="math inline">\(\hat{\theta}\)</span> is just a number.</p>
<p>One way to think of this is to <em>imagine</em> sampling several data sets <span class="math inline">\(X_1,\dots,X_n\)</span>, rather than just one data set. Each data set would give us an MLE. Supppose we collect <span class="math inline">\(J\)</span> datasets, and the <span class="math inline">\(j\)</span>th dataset gives an MLE <span class="math inline">\(\hat{\theta}_j\)</span>. The distribution of the MLE means the distribution of these <span class="math inline">\(\hat{\theta}_j\)</span> values. Essentially it tells us what a histogram of the <span class="math inline">\(\hat{\theta}_j\)</span> values would look like. This distribution is often called the “sampling distribution” of the MLE to emphasise that it is the distribution one would get when sampling many different data sets. The concrete examples given below help illustrate this key idea.</p>
</div>
<div id="implication-for-estimation-error" class="section level3">
<h3>Implication for estimation error</h3>
<p>Notice that, because the mean of the sampling distribution of the MLE is the true value (<span class="math inline">\(\theta_0\)</span>), the variance of the sampling distribution tells us how far we might expect the MLE to lie from the true value. Specifically the variance is, by definition, the expected squared distance of the MLE from the true value <span class="math inline">\(\theta_0\)</span>. Thus the standard deviation (square root of variance) gives the root mean squared error (RMSE) of the MLE.</p>
<p>For i.i.d. data the Fisher information <span class="math inline">\(I_n(\theta)=nI(\theta)\)</span> and so increases linearly with <span class="math inline">\(n\)</span> (see notes above). Consequently the variance decreases linearly with <span class="math inline">\(n\)</span> and the RMSE decreases with <span class="math inline">\(n^0.5\)</span>. Thus, for example, to halve the RMSE we need to multiply sample size by 4.</p>
<p>This is a fundamental idea in statistics: for i.i.d. data (and under regularity conditions) estimation error in the MLE decreases as the <em>square root</em> of the sample size.</p>
</div>
<div id="a-slightly-more-precise-statement" class="section level3">
<h3>A slightly more precise statement</h3>
<p>For those that dislike the vagueness of the statement “for <span class="math inline">\(n\)</span> sufficiently large”, the result can be written more formally as a limiting result as <span class="math inline">\(n \rightarrow \infty\)</span>:</p>
<p><span class="math display">\[I_n(\theta_0)^{-0.5} (\hat{\theta} - \theta_0) \rightarrow N(0,1) \text{ as } n \rightarrow \infty\]</span></p>
<p>This kind of result, where sample size tends to infinity, is often referred to as an “asymptotic” result in statistics. So the result gives the “asymptotic sampling distribution of the MLE”.</p>
<p>While mathematically more precise, this way of writing the result is perhaps less intutive than the approximate statement above.</p>
</div>
</div>
<div id="example-1-bernoulli-proportion" class="section level2">
<h2>Example 1: Bernoulli Proportion</h2>
<p>Assume we observe i.i.d. samples <span class="math inline">\(X_1,\ldots,X_n\)</span> drawn from a Bernoulli<span class="math inline">\((p)\)</span> distribution with true parameter <span class="math inline">\(p=p_0\)</span>. The log-likelihood is: <span class="math display">\[\ell(p; X_1,\dots,X_n) = \sum_{i=1}^n [X_i\log{p} + (1-X_i)\log(1-p)]\]</span> Setting the derivative equal to zero, we obtain:<br />
<span class="math display">\[\frac{d}{dp}\ell(p;X_1,\dots,X_n) = \sum_{i=1}^n 
\left[ \frac{X_i}{p} - \frac{(1-X_i)}{1-p}\right].\]</span><br />
Setting this derivative to 0 and solving for <span class="math inline">\(p\)</span>, gives that the MLE is the sample mean: <span class="math inline">\(\hat{p} = (1/n)\sum_{i=1}^n X_i\)</span>.</p>
<p>The second derivative with respect to <span class="math inline">\(p\)</span> is:<br />
<span class="math display">\[\frac{d^2}{dp^2} \ell(p; X_1,\dots,X_n) = \sum_{i=1}^n 
\left[ -\frac{X_i}{p^2} - \frac{(1-X_i)}{(1-p)^2} \right]\]</span></p>
<p>The Fisher information (for all observations) is therefore: <span class="math display">\[I_{n}(p) = E\left[-\frac{d^2}{dp^2}\ell(p)\right] = \sum_{i=1}^n  \left[ -\frac{E[X_i]}{p^2} - \frac{(1-E[X_i])}{(1-p)^2} \right] = \frac{n}{p(1-p)}.\]</span> Notice that, as expected from the general result <span class="math inline">\(I_n(p)=nI_1(p)\)</span>, <span class="math inline">\(I_n(p)\)</span> increases linearly with <span class="math inline">\(n\)</span>.</p>
<p>From the main result, we have that (for large <span class="math inline">\(n\)</span>), <span class="math inline">\(\hat{p}\)</span> is approximately <span class="math inline">\(N\left(p,\frac{p(1-p)}{n}\right)\)</span>. We illustrate this approximation in the simulation below.</p>
<p>The simulation samples <span class="math inline">\(J=7000\)</span> sets of data <span class="math inline">\(X_1,\dots,X_n\)</span>. In each sample, we have <span class="math inline">\(n=100\)</span> draws from a Bernoulli distribution with true parameter <span class="math inline">\(p_0=0.4\)</span>. We compute the MLE separately for each sample and plot a histogram of these 7000 MLEs. On top of this histogram, we plot the density of the theoretical asymptotic sampling distribution as a solid line.</p>
<pre class="r"><code>num.iterations         &lt;- 7000
p.truth                &lt;- 0.4
num.samples.per.iter   &lt;- 100
samples                &lt;- numeric(num.iterations)
for(iter in seq_len(num.iterations)) {
  samples[iter] &lt;- mean(rbinom(num.samples.per.iter, 1, p.truth))
}
hist(samples, freq=F)
curve(dnorm(x, mean=p.truth,sd=sqrt((p.truth*(1-p.truth)/num.samples.per.iter) )), .25, .55, lwd=2, xlab = &quot;&quot;, ylab = &quot;&quot;, add = T)</code></pre>
<p><img src="asymptotic_normality_mle_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="example-2-poisson-mean" class="section level2">
<h2>Example 2: Poisson Mean</h2>
<p>Assume we observe i.i.d. samples <span class="math inline">\(X_1,\ldots,X_n\)</span> drawn from a Poisson<span class="math inline">\((\lambda)\)</span> distribution with true parameter <span class="math inline">\(\lambda=\lambda_0\)</span>. The log-likelihood is:</p>
<p><span class="math display">\[ \ell(\lambda; X_1,\ldots,X_n) = \sum_{i=1}^n -\lambda + X_i\log(\lambda) + \log(X_i!).\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\lambda\)</span>, setting it equal to zero, and solving for <span class="math inline">\(\lambda\)</span> gives the mle as the sample mean, <span class="math inline">\(\hat{\lambda} = \frac{1}{n}\sum_{i=1}^{n}X_i\)</span>. The Fisher information is:</p>
<p><span class="math display">\[ I_{n}(\lambda) = E_{\lambda}\left[-\frac{d^2}{d\lambda^2}\ell(\lambda)\right] = \sum_{i=1}^n E[X_{i}/\lambda^2] = \frac{n}{\lambda}\]</span></p>
<p>The main result says that (for large <span class="math inline">\(n\)</span>), <span class="math inline">\(\hat{\lambda}\)</span> is approximately <span class="math inline">\(N\left(\lambda,\frac{1}{n\lambda}\right)\)</span>. We illustrate this by simulation again:</p>
<pre class="r"><code>num.iterations         &lt;- 7000
lambda.truth           &lt;- 0.8
num.samples.per.iter   &lt;- 100
samples                &lt;- numeric(num.iterations)
for(iter in seq_len(num.iterations)) {
  samples[iter] &lt;- mean(rpois(num.samples.per.iter, lambda.truth))
}
hist(samples, freq=F)
curve(dnorm(x, mean=lambda.truth,sd=sqrt(lambda.truth/num.samples.per.iter) ), 0.4, 1.2, lwd=2, xlab = &quot;&quot;, ylab = &quot;&quot;, add = T)</code></pre>
<p><img src="asymptotic_normality_mle_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>

<hr>
<p>
    This site was created with <a href="http://rmarkdown.rstudio.com">R Markdown</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
